{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
    "from keras.models import Model\n",
    "from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from keras.callbacks import EarlyStopping\n",
    "## 模型的保存和导入\n",
    "from keras.models import load_model\n",
    "# 保存训练好的Tokenizer，和导入\n",
    "import pickle\n",
    "import jieba.analyse\n",
    "import jieba\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 导入已经训练好的模型\n",
    "tnews = load_model('LSTM_tnews_better_model.h5')\n",
    "thucnews = load_model('LSTM_thucnews_model.h5')\n",
    "inews = load_model('inews_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\yinzh\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.700 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "# tnews\n",
    "# 读取训练集\n",
    "train_file = open(\"./toutiao_category_train.txt\", encoding='utf-8')\n",
    "train_collection = train_file.readlines()\n",
    "train_data = []\n",
    "for i in train_collection:\n",
    "    i = i.split(\"_!_\")\n",
    "    i[4] = i[4].strip()\n",
    "    i[4] = \" \".join(i[4].split(\",\")) + \" \" + \" \".join(jieba.cut(re.sub(r'[^\\w\\s]','',i[3].strip()), cut_all=False))\n",
    "    i.append(len(i[4].split(\" \")))\n",
    "    i.pop(0)\n",
    "    i.pop(2)\n",
    "    train_data.append(i)\n",
    "train_df = pd.DataFrame(train_data)\n",
    "train_df.columns = [\"label_index\", \"label\", \"cutword\", \"cutwordnum\"]\n",
    "# 读取测试集\n",
    "test_file = open(\"./toutiao_category_test.txt\", encoding='utf-8')\n",
    "test_collection = test_file.readlines()\n",
    "test_data = []\n",
    "for i in test_collection:\n",
    "    i =i.split(\"_!_\")\n",
    "    i[4] = i[4].strip()\n",
    "    i[4] = \" \".join(i[4].split(\",\")) + \" \" + \" \".join(jieba.cut(re.sub(r'[^\\w\\s]','',i[3].strip()), cut_all=False))\n",
    "    i.append(len(i[4].split(\" \")))\n",
    "    i.pop(0)\n",
    "    i.pop(2)\n",
    "    test_data.append(i)\n",
    "test_df = pd.DataFrame(test_data)\n",
    "test_df.columns = [\"label_index\", \"label\", \"cutword\", \"cutwordnum\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.78      0.78      2972\n",
      "           1       0.91      0.90      0.91      5292\n",
      "           2       0.82      0.74      0.78      4734\n",
      "           3       0.83      0.85      0.84      4101\n",
      "           4       0.86      0.85      0.86      5886\n",
      "           5       0.76      0.77      0.76      4071\n",
      "           6       0.89      0.91      0.90      4183\n",
      "           7       0.87      0.90      0.89      2579\n",
      "           8       0.79      0.86      0.82      3516\n",
      "           9       0.92      0.93      0.92      5544\n",
      "          10       0.75      0.76      0.76       937\n",
      "          11       0.85      0.85      0.85      6292\n",
      "          12       0.76      0.75      0.75      3325\n",
      "          13       0.79      0.78      0.79      3972\n",
      "          14       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.84     57404\n",
      "   macro avg       0.77      0.78      0.77     57404\n",
      "weighted avg       0.84      0.84      0.84     57404\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1439: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "## 对数据集的标签数据进行重新编码\n",
    "train_y = train_df.label\n",
    "test_y = test_df.label\n",
    "le = LabelEncoder()\n",
    "train_y = le.fit_transform(train_y).reshape(-1,1)\n",
    "test_y = le.transform(test_y).reshape(-1,1)\n",
    "\n",
    "## 对数据集的标签数据进行one-hot编码\n",
    "ohe = OneHotEncoder()\n",
    "train_y = ohe.fit_transform(train_y).toarray()\n",
    "test_y = ohe.transform(test_y).toarray()\n",
    "\n",
    "max_words = 5000\n",
    "max_len = 600\n",
    "tok = Tokenizer(num_words=max_words)  ## 使用的最大词语数为5000\n",
    "tok.fit_on_texts(train_df.cutword)\n",
    "\n",
    "# 使用tok对验证数据集重新预处理\n",
    "test_seq = tok.texts_to_sequences(test_df.cutword)\n",
    "# 将每个序列调整为相同的长度\n",
    "test_seq_mat = sequence.pad_sequences(test_seq,maxlen=max_len)\n",
    "# 对验证集进行预测\n",
    "test_pre = tnews.predict(test_seq_mat)\n",
    "print(metrics.classification_report(np.argmax(test_pre,axis=1),np.argmax(test_y,axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tnews_file = open('tnews_result.txt', 'w')\n",
    "for i in np.argmax(test_pre,axis=1):\n",
    "    t = -1\n",
    "    if i == 0:\n",
    "        t = 115\n",
    "    elif i == 1:\n",
    "        t = 107\n",
    "    elif i == 2:\n",
    "        t = 101\n",
    "    elif i == 3:\n",
    "        t = 108\n",
    "    elif i == 4:\n",
    "        t = 102\n",
    "    elif i == 5:\n",
    "        t = 104\n",
    "    elif i == 6:\n",
    "        t = 116\n",
    "    elif i == 7:\n",
    "        t = 106\n",
    "    elif i == 8:\n",
    "        t = 110\n",
    "    elif i == 9:\n",
    "        t = 103\n",
    "    elif i == 10:\n",
    "        t = 100\n",
    "    elif i == 11:\n",
    "        t = 109\n",
    "    elif i == 12:\n",
    "        t = 112\n",
    "    elif i == 13:\n",
    "        t = 113\n",
    "    else:\n",
    "        print(\"Error\")\n",
    "        break\n",
    "    tnews_file.write(\"{}\\n\".format(t))\n",
    "tnews_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# thucnews\n",
    "# 读取训练集\n",
    "train_file = open(\"./thucnews_train.txt\", encoding='utf-8')\n",
    "train_collection = train_file.readlines()\n",
    "train_data = []\n",
    "for i in train_collection:\n",
    "    i = i.split(\"_!_\")\n",
    "    if len(i) < 4:\n",
    "        continue\n",
    "    i[3] = \" \".join(jieba.analyse.textrank(i[3], topK=30))\n",
    "    i.append(len(i[3].split()))\n",
    "    i.pop(2) # 去除ID\n",
    "    train_data.append(i)\n",
    "train_df = pd.DataFrame(train_data)\n",
    "train_df.columns = [\"label_index\", \"label\", \"cutword\", \"cutwordnum\"]\n",
    "\n",
    "# 读取测试集\n",
    "test_file = open(\"./thucnews_test.txt\", encoding='utf-8')\n",
    "test_collection = test_file.readlines()\n",
    "test_data = []\n",
    "for i in test_collection:\n",
    "    i = i.split(\"_!_\")\n",
    "    if len(i) < 4:\n",
    "        continue\n",
    "    i[3] = \" \".join(jieba.analyse.textrank(i[3], topK=30))\n",
    "    i.append(len(i[3].split()))\n",
    "    i.pop(2) # 去除ID\n",
    "    test_data.append(i)\n",
    "test_df = pd.DataFrame(test_data)\n",
    "test_df.columns = [\"label_index\", \"label\", \"cutword\", \"cutwordnum\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.97      0.96       671\n",
      "           1       0.92      0.77      0.84       530\n",
      "           2       0.82      0.71      0.76       168\n",
      "           3       0.72      0.82      0.77        38\n",
      "           4       0.84      0.79      0.81       102\n",
      "           5       0.86      0.87      0.87       230\n",
      "           6       0.64      0.60      0.62        60\n",
      "           7       0.73      0.87      0.80       247\n",
      "           8       0.15      0.50      0.24         4\n",
      "           9       0.72      0.83      0.77        98\n",
      "          10       0.72      0.80      0.76       224\n",
      "          11       0.86      0.89      0.88       801\n",
      "          12       0.91      0.86      0.89       827\n",
      "          13       0.75      0.78      0.76       180\n",
      "\n",
      "    accuracy                           0.86      4180\n",
      "   macro avg       0.76      0.79      0.77      4180\n",
      "weighted avg       0.86      0.86      0.86      4180\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## 对数据集的标签数据进行重新编码\n",
    "train_y = train_df.label\n",
    "test_y = test_df.label\n",
    "le = LabelEncoder()\n",
    "train_y = le.fit_transform(train_y).reshape(-1,1)\n",
    "test_y = le.transform(test_y).reshape(-1,1)\n",
    "\n",
    "## 对数据集的标签数据进行one-hot编码\n",
    "ohe = OneHotEncoder()\n",
    "train_y = ohe.fit_transform(train_y).toarray()\n",
    "test_y = ohe.transform(test_y).toarray()\n",
    "\n",
    "max_words = 5000\n",
    "max_len = 600\n",
    "tok = Tokenizer(num_words=max_words)  ## 使用的最大词语数为5000\n",
    "tok.fit_on_texts(train_df.cutword)\n",
    "\n",
    "# 使用tok对验证数据集重新预处理\n",
    "test_seq = tok.texts_to_sequences(test_df.cutword)\n",
    "# 将每个序列调整为相同的长度\n",
    "test_seq_mat = sequence.pad_sequences(test_seq,maxlen=max_len)\n",
    "# 对验证集进行预测\n",
    "test_pre = thucnews.predict(test_seq_mat)\n",
    "print(metrics.classification_report(np.argmax(test_pre,axis=1),np.argmax(test_y,axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "0\n",
      "0\n",
      "1\n",
      "7\n",
      "0\n",
      "13\n",
      "4\n",
      "5\n",
      "0\n",
      "0\n",
      "7\n",
      "7\n",
      "0\n",
      "10\n",
      "10\n",
      "12\n",
      "11\n",
      "1\n",
      "10\n",
      "1\n",
      "13\n",
      "11\n",
      "11\n",
      "0\n",
      "5\n",
      "0\n",
      "13\n",
      "0\n",
      "12\n",
      "1\n",
      "6\n",
      "11\n",
      "11\n",
      "7\n",
      "12\n",
      "2\n",
      "12\n",
      "12\n",
      "10\n",
      "0\n",
      "1\n",
      "11\n",
      "12\n",
      "1\n",
      "12\n",
      "11\n",
      "11\n",
      "0\n",
      "0\n",
      "12\n",
      "11\n",
      "12\n",
      "0\n",
      "4\n",
      "0\n",
      "11\n",
      "4\n",
      "11\n",
      "11\n",
      "0\n",
      "0\n",
      "11\n",
      "12\n",
      "7\n",
      "2\n",
      "12\n",
      "2\n",
      "13\n",
      "11\n",
      "0\n",
      "11\n",
      "0\n",
      "11\n",
      "11\n",
      "0\n",
      "0\n",
      "1\n",
      "13\n",
      "1\n",
      "7\n",
      "0\n",
      "0\n",
      "12\n",
      "2\n",
      "0\n",
      "1\n",
      "5\n",
      "0\n",
      "11\n",
      "11\n",
      "11\n",
      "1\n",
      "1\n",
      "0\n",
      "7\n",
      "0\n",
      "11\n",
      "2\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# count = 0\n",
    "# for i in np.argmax(test_y,axis=1):\n",
    "#     print(i)\n",
    "#     count += 1\n",
    "#     if count == 100:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "thucnews_file = open('thucnews_result.txt', 'w')\n",
    "for i in np.argmax(test_pre,axis=1):\n",
    "    thucnews_file.write(\"{}\\n\".format(i))\n",
    "thucnews_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取训练集\n",
    "train_file = open(\"./inews_train.txt\", encoding='utf-8')\n",
    "train_collection = train_file.readlines()\n",
    "train_data = []\n",
    "for i in train_collection:\n",
    "    i = i.split(\"_!_\")\n",
    "    # 提取30个关键词 + \" \" + 标题分词\n",
    "    i[2] = \" \".join(jieba.analyse.textrank(i[3], topK=30)) + \" \" + \" \".join(jieba.cut(re.sub(r'[^\\w\\s]','',i[2].strip()), cut_all=False))\n",
    "    i.append(len(i[2].split()))\n",
    "    i.pop(1) # 去除ID\n",
    "    i.pop(2)\n",
    "    train_data.append(i)\n",
    "train_df = pd.DataFrame(train_data)\n",
    "train_df.columns = [\"label\", \"cutword\", \"cutwordnum\"]\n",
    "\n",
    "# 读取测试集\n",
    "test_file = open(\"./inews_test.txt\", encoding='utf-8')\n",
    "test_collection = test_file.readlines()\n",
    "test_data = []\n",
    "for i in test_collection:\n",
    "    i = i.split(\"_!_\")\n",
    "    i[2] = \" \".join(jieba.analyse.textrank(i[3], topK=30)) + \" \" + \" \".join(jieba.cut(re.sub(r'[^\\w\\s]','',i[2].strip()), cut_all=False))\n",
    "    i.append(len(i[2].split()))\n",
    "    i.pop(1) # 去除ID\n",
    "    i.pop(2)\n",
    "    test_data.append(i)\n",
    "test_df = pd.DataFrame(test_data)\n",
    "test_df.columns = [\"label\", \"cutword\", \"cutwordnum\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.47      0.54       131\n",
      "           1       0.72      0.80      0.75       444\n",
      "           2       0.82      0.78      0.80       425\n",
      "\n",
      "    accuracy                           0.75      1000\n",
      "   macro avg       0.72      0.68      0.70      1000\n",
      "weighted avg       0.75      0.75      0.74      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## 对数据集的标签数据进行重新编码\n",
    "train_y = train_df.label\n",
    "test_y = test_df.label\n",
    "le = LabelEncoder()\n",
    "train_y = le.fit_transform(train_y).reshape(-1,1)\n",
    "test_y = le.transform(test_y).reshape(-1,1)\n",
    "\n",
    "## 对数据集的标签数据进行one-hot编码\n",
    "ohe = OneHotEncoder()\n",
    "train_y = ohe.fit_transform(train_y).toarray()\n",
    "test_y = ohe.transform(test_y).toarray()\n",
    "\n",
    "max_words = 5000\n",
    "max_len = 600\n",
    "tok = Tokenizer(num_words=max_words)  ## 使用的最大词语数为5000\n",
    "tok.fit_on_texts(train_df.cutword)\n",
    "\n",
    "# 使用tok对验证数据集重新预处理\n",
    "test_seq = tok.texts_to_sequences(test_df.cutword)\n",
    "# 将每个序列调整为相同的长度\n",
    "test_seq_mat = sequence.pad_sequences(test_seq,maxlen=max_len)\n",
    "# 对验证集进行预测\n",
    "test_pre = inews.predict(test_seq_mat)\n",
    "print(metrics.classification_report(np.argmax(test_pre,axis=1),np.argmax(test_y,axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "0\n",
      "2\n",
      "2\n",
      "0\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "0\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "0\n",
      "0\n",
      "2\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# count = 0\n",
    "# for i in np.argmax(test_y,axis=1):\n",
    "#     print(i)\n",
    "#     count += 1\n",
    "#     if count == 100:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "inews_file = open('inews_result.txt', 'w')\n",
    "for i in np.argmax(test_pre,axis=1):\n",
    "    inews_file.write(\"{}\\n\".format(i))\n",
    "inews_file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
