{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
    "import jieba.analyse\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取数据集\n",
    "def load_data():\n",
    "    # 读取训练集\n",
    "    train_file = open(\"./thucnews_train.txt\", encoding='utf-8')\n",
    "    train_collection = train_file.readlines()\n",
    "    train_data = []\n",
    "    for i in train_collection:\n",
    "        i = i.split(\"_!_\")\n",
    "#         print(i[3][:100])\n",
    "#         keywords = []\n",
    "# #         print(i[3])\n",
    "# #         print(type(jieba.analyse.textrank(i[3], topK=30)))\n",
    "#         items = jieba.analyse.textrank(i[3], topK=30) # 提取30个关键词\n",
    "#         for item in items:\n",
    "#             keywords.append(item[0])\n",
    "        if len(i) < 4:\n",
    "            continue\n",
    "        i[3] = \" \".join(jieba.analyse.textrank(i[3], topK=30))\n",
    "        i.append(len(i[3].split()))\n",
    "        i.pop(2) # 去除ID\n",
    "        train_data.append(i)\n",
    "    train_df = pd.DataFrame(train_data)\n",
    "    train_df.columns = [\"label_index\", \"label\", \"cutword\", \"cutwordnum\"]\n",
    "    \n",
    "    # 读取验证集\n",
    "    val_file = open(\"./thucnews_dev.txt\", encoding='utf-8')\n",
    "    val_collection = val_file.readlines()\n",
    "    val_data = []\n",
    "    for i in val_collection:\n",
    "        i = i.split(\"_!_\")\n",
    "        if len(i) < 4:\n",
    "            continue\n",
    "        i[3] = \" \".join(jieba.analyse.textrank(i[3], topK=30))\n",
    "        i.append(len(i[3].split()))\n",
    "        i.pop(2) # 去除ID\n",
    "        val_data.append(i)\n",
    "    val_df = pd.DataFrame(val_data)\n",
    "    val_df.columns = [\"label_index\", \"label\", \"cutword\", \"cutwordnum\"]\n",
    "    \n",
    "    # 读取测试集\n",
    "    test_file = open(\"./thucnews_test.txt\", encoding='utf-8')\n",
    "    test_collection = test_file.readlines()\n",
    "    test_data = []\n",
    "    for i in test_collection:\n",
    "        i = i.split(\"_!_\")\n",
    "        if len(i) < 4:\n",
    "            continue\n",
    "        i[3] = \" \".join(jieba.analyse.textrank(i[3], topK=30))\n",
    "        i.append(len(i[3].split()))\n",
    "        i.pop(2) # 去除ID\n",
    "        test_data.append(i)\n",
    "    test_df = pd.DataFrame(test_data)\n",
    "    test_df.columns = [\"label_index\", \"label\", \"cutword\", \"cutwordnum\"]\n",
    "    \n",
    "    return train_df, val_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\yinzh\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.815 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "train_df, val_df, test_df = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据探索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 查看训练集都有哪些标签\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']\n",
    "plt.figure()\n",
    "sns.countplot(train_df.label)\n",
    "plt.xlabel('label')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "共15种标签，标签分布比较均匀。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 分析训练集中词组数量的分布\n",
    "print(train_df.cutwordnum.describe())\n",
    "plt.figure()\n",
    "plt.hist(train_df.cutwordnum,bins=100)\n",
    "plt.xlabel(\"phrase length\")\n",
    "plt.ylabel(\"frequency\")\n",
    "plt.title(\"train data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "from keras.models import Model\n",
    "from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 对数据集的标签数据进行重新编码\n",
    "train_y = train_df.label\n",
    "val_y = val_df.label\n",
    "test_y = test_df.label\n",
    "le = LabelEncoder()\n",
    "train_y = le.fit_transform(train_y).reshape(-1,1)\n",
    "val_y = le.transform(val_y).reshape(-1,1)\n",
    "test_y = le.transform(test_y).reshape(-1,1)\n",
    "\n",
    "## 对数据集的标签数据进行one-hot编码\n",
    "ohe = OneHotEncoder()\n",
    "train_y = ohe.fit_transform(train_y).toarray()\n",
    "val_y = ohe.transform(val_y).toarray()\n",
    "test_y = ohe.transform(test_y).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用Tokenizer对词组进行编码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 5000\n",
    "max_len = 600\n",
    "tok = Tokenizer(num_words=max_words)  ## 使用的最大词语数为5000\n",
    "tok.fit_on_texts(train_df.cutword)\n",
    "\n",
    "## 使用word_index属性可以看到每次词对应的编码\n",
    "## 使用word_counts属性可以看到每个词对应的频数\n",
    "for ii,iterm in enumerate(tok.word_index.items()):\n",
    "    if ii < 10:\n",
    "        print(iterm)\n",
    "    else:\n",
    "        break\n",
    "print(\"===================\")  \n",
    "for ii,iterm in enumerate(tok.word_counts.items()):\n",
    "    if ii < 10:\n",
    "        print(iterm)\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用tok.texts_to_sequences()将数据转化为序列，并使用sequence.pad_sequences()将每个序列调整为相同的长度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 对每个词编码之后，每句新闻中的每个词就可以用对应的编码表示，即每条新闻可以转变成一个向量了：\n",
    "train_seq = tok.texts_to_sequences(train_df.cutword)\n",
    "val_seq = tok.texts_to_sequences(val_df.cutword)\n",
    "test_seq = tok.texts_to_sequences(test_df.cutword)\n",
    "## 将每个序列调整为相同的长度\n",
    "train_seq_mat = sequence.pad_sequences(train_seq,maxlen=max_len)\n",
    "val_seq_mat = sequence.pad_sequences(val_seq,maxlen=max_len)\n",
    "test_seq_mat = sequence.pad_sequences(test_seq,maxlen=max_len)\n",
    "\n",
    "print(train_seq_mat.shape)\n",
    "print(val_seq_mat.shape)\n",
    "print(test_seq_mat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.label.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_df.label.drop_duplicates())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "建立LSTM模型并训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 定义LSTM模型\n",
    "inputs = Input(name='inputs',shape=[max_len])\n",
    "## Embedding(词汇表大小,batch大小,每个新闻的词长)\n",
    "layer = Embedding(max_words+1,128,input_length=max_len)(inputs)\n",
    "layer = LSTM(64)(layer)\n",
    "layer = Dense(32,activation=\"relu\",name=\"FC1\")(layer)\n",
    "# layer = Dropout(0.5)(layer)\n",
    "layer = Dense(14,activation=\"softmax\",name=\"FC2\")(layer)\n",
    "model = Model(inputs=inputs,outputs=layer)\n",
    "model.summary()\n",
    "model.compile(loss=\"categorical_crossentropy\",optimizer=RMSprop(),metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 模型训练\n",
    "model_fit = model.fit(train_seq_mat,train_y,batch_size=512,epochs=10,\n",
    "                      validation_data=(val_seq_mat,val_y),\n",
    "                      callbacks=[EarlyStopping(monitor='val_loss',min_delta=0.0001)] ## 当val-loss不再提升时停止训练\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 对测试集进行预测\n",
    "test_pre = model.predict(test_seq_mat)\n",
    "\n",
    "## 评价预测效果，计算混淆矩阵\n",
    "confm = metrics.confusion_matrix(np.argmax(test_pre,axis=1),np.argmax(test_y,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Labname =  list(train_df.label.drop_duplicates())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 混淆矩阵可视化\n",
    "plt.figure(figsize=(8,8))\n",
    "sns.heatmap(confm.T, square=True, annot=True,\n",
    "            fmt='d', cbar=False,linewidths=.8,\n",
    "            cmap=\"YlGnBu\")\n",
    "plt.xlabel('True label',size = 14)\n",
    "plt.ylabel('Predicted label',size = 14)\n",
    "plt.xticks(np.arange(len(Labname))+0.5,Labname,rotation=90)\n",
    "plt.yticks(np.arange(len(Labname))+0.3,Labname,rotation=0)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(metrics.classification_report(np.argmax(test_pre,axis=1),np.argmax(test_y,axis=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用时1小时，测试集准确率86%，相当不错的准确率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存训练好的Tokenizer，和导入\n",
    "import pickle\n",
    "# saving\n",
    "with open('tok_thucnews.pickle', 'wb') as handle:\n",
    "    pickle.dump(tok, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# # loading\n",
    "# with open('tok_thucnews.pickle', 'rb') as handle:\n",
    "#     tok = pickle.load(handle)\n",
    "# ## 使用word_index属性可以看到每次词对应的编码\n",
    "# ## 使用word_counts属性可以看到每个词对应的频数\n",
    "# for ii,iterm in enumerate(tok.word_index.items()):\n",
    "#     if ii < 10:\n",
    "#         print(iterm)\n",
    "#     else:\n",
    "#         break\n",
    "# print(\"===================\")  \n",
    "# for ii,iterm in enumerate(tok.word_counts.items()):\n",
    "#     if ii < 10:\n",
    "#         print(iterm)\n",
    "#     else:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 模型的保存和导入\n",
    "from keras.models import load_model\n",
    "# 保存模型\n",
    "model.save('LSTM_thucnews_model.h5')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 导入已经训练好的模型\n",
    "# model = load_model('my_model.h5')\n",
    "# ## 使用tok对验证数据集重新预处理\n",
    "# val_seq = tok.texts_to_sequences(val_df.cutword)\n",
    "# ## 将每个序列调整为相同的长度\n",
    "# val_seq_mat = sequence.pad_sequences(val_seq,maxlen=max_len)\n",
    "# ## 对验证集进行预测\n",
    "# val_pre = model.predict(val_seq_mat)\n",
    "# print(metrics.classification_report(np.argmax(val_pre,axis=1),np.argmax(val_y,axis=1)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
